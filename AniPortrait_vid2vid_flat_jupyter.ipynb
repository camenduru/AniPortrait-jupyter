{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/AniPortrait-jupyter/blob/main/AniPortrait_vid2vid_flat_jupyter.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b dev https://github.com/camenduru/AniPortrait\n",
        "%cd /content/AniPortrait\n",
        "\n",
        "!apt install espeak-ng aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/AniPortrait/resolve/main/audio2mesh.pt -d /content/AniPortrait/pretrained_model -o audio2mesh.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/AniPortrait/resolve/main/denoising_unet.pth -d /content/AniPortrait/pretrained_model -o denoising_unet.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/AniPortrait/resolve/main/motion_module.pth -d /content/AniPortrait/pretrained_model -o motion_module.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/AniPortrait/resolve/main/pose_guider.pth -d /content/AniPortrait/pretrained_model -o pose_guider.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/AniPortrait/resolve/main/reference_unet.pth -d /content/AniPortrait/pretrained_model -o reference_unet.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/champ/resolve/main/champ/denoising_unet.pth -d /content/AniPortrait/pretrained_model -o denoising_unet.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/champ/resolve/main/champ/guidance_encoder_depth.pth -d /content/AniPortrait/pretrained_model -o guidance_encoder_depth.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/champ/resolve/main/champ/guidance_encoder_dwpose.pth -d /content/AniPortrait/pretrained_model -o guidance_encoder_dwpose.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/champ/resolve/main/champ/guidance_encoder_normal.pth -d /content/AniPortrait/pretrained_model -o guidance_encoder_normal.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/champ/resolve/main/champ/guidance_encoder_semantic_map.pth -d /content/AniPortrait/pretrained_model -o guidance_encoder_semantic_map.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/champ/resolve/main/champ/motion_module.pth -d /content/AniPortrait/pretrained_model -o motion_module.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/champ/resolve/main/champ/reference_unet.pth -d /content/AniPortrait/pretrained_model -o reference_unet.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/champ/raw/main/image_encoder/config.json -d /content/AniPortrait/pretrained_model/image_encoder -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/champ/resolve/main/image_encoder/pytorch_model.bin -d /content/AniPortrait/pretrained_model/image_encoder -o pytorch_model.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/champ/raw/main/sd-vae-ft-mse/config.json -d /content/AniPortrait/pretrained_model/sd-vae-ft-mse -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/champ/resolve/main/sd-vae-ft-mse/diffusion_pytorch_model.bin -d /content/AniPortrait/pretrained_model/sd-vae-ft-mse -o diffusion_pytorch_model.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/champ/raw/main/stable-diffusion-v1-5/feature_extractor/preprocessor_config.json -d /content/AniPortrait/pretrained_model/stable-diffusion-v1-5/feature_extractor -o preprocessor_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/champ/raw/main/stable-diffusion-v1-5/model_index.json -d /content/AniPortrait/pretrained_model/stable-diffusion-v1-5 -o model_index.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/champ/raw/main/stable-diffusion-v1-5/unet/config.json -d /content/AniPortrait/pretrained_model/stable-diffusion-v1-5/unet -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/champ/resolve/main/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin -d /content/AniPortrait/pretrained_model/stable-diffusion-v1-5/unet -o diffusion_pytorch_model.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/champ/raw/main/stable-diffusion-v1-5/v1-inference.yaml -d /content/AniPortrait/pretrained_model/stable-diffusion-v1-5 -o v1-inference.yaml\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/facebook/wav2vec2-base-960h/raw/main/config.json -d /content/AniPortrait/pretrained_model/wav2vec2-base-960h -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/facebook/wav2vec2-base-960h/raw/main/feature_extractor_config.json -d /content/AniPortrait/pretrained_model/wav2vec2-base-960h -o feature_extractor_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/facebook/wav2vec2-base-960h/raw/main/preprocessor_config.json -d /content/AniPortrait/pretrained_model/wav2vec2-base-960h -o preprocessor_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/pytorch_model.bin -d /content/AniPortrait/pretrained_model/wav2vec2-base-960h -o pytorch_model.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/facebook/wav2vec2-base-960h/raw/main/special_tokens_map.json -d /content/AniPortrait/pretrained_model/wav2vec2-base-960h -o special_tokens_map.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/tf_model.h5 -d /content/AniPortrait/pretrained_model/wav2vec2-base-960h -o tf_model.h5\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/facebook/wav2vec2-base-960h/raw/main/tokenizer_config.json -d /content/AniPortrait/pretrained_model/wav2vec2-base-960h -o tokenizer_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/facebook/wav2vec2-base-960h/raw/main/vocab.json -d /content/AniPortrait/pretrained_model/wav2vec2-base-960h -o vocab.json\n",
        "\n",
        "!pip install -q imageio-ffmpeg==0.4.9 ffmpeg-python==0.2.0 av==11.0.0 omegaconf==2.2.3 diffusers==0.24.0 mediapipe==0.10.11 einops==0.4.1 accelerate==0.21.0 xformers==0.0.25 librosa==0.9.2\n",
        "\n",
        "%cd /content/AniPortrait\n",
        "\n",
        "import os\n",
        "import ffmpeg\n",
        "from datetime import datetime\n",
        "from pathlib import Path as MyPath\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "from diffusers import AutoencoderKL, DDIMScheduler\n",
        "from einops import repeat\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from transformers import CLIPVisionModelWithProjection\n",
        "\n",
        "from src.models.pose_guider import PoseGuider\n",
        "from src.models.unet_2d_condition import UNet2DConditionModel\n",
        "from src.models.unet_3d import UNet3DConditionModel\n",
        "from src.pipelines.pipeline_pose2vid_long import Pose2VideoPipeline\n",
        "from src.utils.util import get_fps, read_frames, save_videos_grid\n",
        "\n",
        "from src.utils.mp_utils  import LMKExtractor\n",
        "from src.utils.draw_util import FaceMeshVisualizer\n",
        "from src.utils.pose_util import project_points_with_trans\n",
        "\n",
        "class ChatGPTConfig:\n",
        "    def __init__(self):\n",
        "        self.pretrained_base_model_path = './pretrained_model/stable-diffusion-v1-5'\n",
        "        self.pretrained_vae_path = './pretrained_model/sd-vae-ft-mse'\n",
        "        self.image_encoder_path = './pretrained_model/image_encoder'\n",
        "        self.denoising_unet_path = \"./pretrained_model/denoising_unet.pth\"\n",
        "        self.reference_unet_path = \"./pretrained_model/reference_unet.pth\"\n",
        "        self.pose_guider_path = \"./pretrained_model/pose_guider.pth\"\n",
        "        self.motion_module_path = \"./pretrained_model/motion_module.pth\"\n",
        "        self.inference_config = \"./configs/inference/inference_v2.yaml\"\n",
        "        self.weight_dtype = 'fp16'\n",
        "        self.test_cases = {\n",
        "            \"./configs/inference/ref_images/Aragaki.png\": [\"./configs/inference/video/Aragaki_song.mp4\"]\n",
        "        }\n",
        "config = ChatGPTConfig()\n",
        "\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.W = 512\n",
        "        self.H = 512\n",
        "        self.L = 64\n",
        "        self.seed = 42\n",
        "        self.cfg = 3.5\n",
        "        self.steps = 25\n",
        "        self.fps = 30\n",
        "\n",
        "args = Args()\n",
        "\n",
        "if config.weight_dtype == \"fp16\":\n",
        "    weight_dtype = torch.float16\n",
        "else:\n",
        "    weight_dtype = torch.float32\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained(\n",
        "    config.pretrained_vae_path,\n",
        ").to(\"cuda\", dtype=weight_dtype)\n",
        "\n",
        "reference_unet = UNet2DConditionModel.from_pretrained(\n",
        "    config.pretrained_base_model_path,\n",
        "    subfolder=\"unet\",\n",
        ").to(dtype=weight_dtype, device=\"cuda\")\n",
        "\n",
        "inference_config_path = config.inference_config\n",
        "infer_config = OmegaConf.load(inference_config_path)\n",
        "denoising_unet = UNet3DConditionModel.from_pretrained_2d(\n",
        "    config.pretrained_base_model_path,\n",
        "    config.motion_module_path,\n",
        "    subfolder=\"unet\",\n",
        "    unet_additional_kwargs=infer_config.unet_additional_kwargs,\n",
        ").to(dtype=weight_dtype, device=\"cuda\")\n",
        "\n",
        "pose_guider = PoseGuider(noise_latent_channels=320, use_ca=True).to(device=\"cuda\", dtype=weight_dtype) # not use cross attention\n",
        "\n",
        "image_enc = CLIPVisionModelWithProjection.from_pretrained(\n",
        "    config.image_encoder_path\n",
        ").to(dtype=weight_dtype, device=\"cuda\")\n",
        "\n",
        "sched_kwargs = OmegaConf.to_container(infer_config.noise_scheduler_kwargs)\n",
        "scheduler = DDIMScheduler(**sched_kwargs)\n",
        "\n",
        "generator = torch.manual_seed(args.seed)\n",
        "\n",
        "width, height = args.W, args.H\n",
        "\n",
        "# load pretrained weights\n",
        "denoising_unet.load_state_dict(\n",
        "    torch.load(config.denoising_unet_path, map_location=\"cpu\"),\n",
        "    strict=False,\n",
        ")\n",
        "reference_unet.load_state_dict(\n",
        "    torch.load(config.reference_unet_path, map_location=\"cpu\"),\n",
        ")\n",
        "pose_guider.load_state_dict(\n",
        "    torch.load(config.pose_guider_path, map_location=\"cpu\"),\n",
        ")\n",
        "\n",
        "pipe = Pose2VideoPipeline(\n",
        "    vae=vae,\n",
        "    image_encoder=image_enc,\n",
        "    reference_unet=reference_unet,\n",
        "    denoising_unet=denoising_unet,\n",
        "    pose_guider=pose_guider,\n",
        "    scheduler=scheduler,\n",
        ")\n",
        "pipe = pipe.to(\"cuda\", dtype=weight_dtype)\n",
        "\n",
        "date_str = datetime.now().strftime(\"%Y%m%d\")\n",
        "time_str = datetime.now().strftime(\"%H%M\")\n",
        "save_dir_name = f\"{time_str}--seed_{args.seed}-{args.W}x{args.H}\"\n",
        "\n",
        "save_dir = Path(f\"output/{date_str}/{save_dir_name}\")\n",
        "save_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "\n",
        "lmk_extractor = LMKExtractor()\n",
        "vis = FaceMeshVisualizer(forehead_edge=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ref_image_path = './configs/inference/ref_images/Aragaki.png'\n",
        "source_video_path = './configs/inference/video/Aragaki_song.mp4'\n",
        "\n",
        "ref_name = Path(ref_image_path).stem\n",
        "pose_name = Path(source_video_path).stem\n",
        "\n",
        "ref_image_pil = Image.open(ref_image_path).convert(\"RGB\")\n",
        "ref_image_np = cv2.cvtColor(np.array(ref_image_pil), cv2.COLOR_RGB2BGR)\n",
        "ref_image_np = cv2.resize(ref_image_np, (args.H, args.W))\n",
        "\n",
        "face_result = lmk_extractor(ref_image_np)\n",
        "assert face_result is not None, \"Can not detect a face in the reference image.\"\n",
        "lmks = face_result['lmks'].astype(np.float32)\n",
        "ref_pose = vis.draw_landmarks((ref_image_np.shape[1], ref_image_np.shape[0]), lmks, normed=True)\n",
        "\n",
        "source_images = read_frames(source_video_path)\n",
        "src_fps = get_fps(source_video_path)\n",
        "print(f\"source video has {len(source_images)} frames, with {src_fps} fps\")\n",
        "pose_transform = transforms.Compose(\n",
        "    [transforms.Resize((height, width)), transforms.ToTensor()]\n",
        ")\n",
        "\n",
        "step = 1\n",
        "if src_fps == 60:\n",
        "    src_fps = 30\n",
        "    step = 2\n",
        "\n",
        "pose_trans_list = []\n",
        "verts_list = []\n",
        "bs_list = []\n",
        "src_tensor_list = []\n",
        "for src_image_pil in source_images[: args.L*step: step]:\n",
        "    src_tensor_list.append(pose_transform(src_image_pil))\n",
        "    src_img_np = cv2.cvtColor(np.array(src_image_pil), cv2.COLOR_RGB2BGR)\n",
        "    frame_height, frame_width, _ = src_img_np.shape\n",
        "    src_img_result = lmk_extractor(src_img_np)\n",
        "    if src_img_result is None:\n",
        "        break\n",
        "    pose_trans_list.append(src_img_result['trans_mat'])\n",
        "    verts_list.append(src_img_result['lmks3d'])\n",
        "    bs_list.append(src_img_result['bs'])\n",
        "\n",
        "\n",
        "pose_arr = np.array(pose_trans_list)\n",
        "verts_arr = np.array(verts_list)\n",
        "bs_arr = np.array(bs_list)\n",
        "min_bs_idx = np.argmin(bs_arr.sum(1))\n",
        "\n",
        "# face retarget\n",
        "verts_arr = verts_arr - verts_arr[min_bs_idx] + face_result['lmks3d']\n",
        "# project 3D mesh to 2D landmark\n",
        "projected_vertices = project_points_with_trans(verts_arr, pose_arr, [frame_height, frame_width])\n",
        "\n",
        "pose_list = []\n",
        "for i, verts in enumerate(projected_vertices):\n",
        "    lmk_img = vis.draw_landmarks((frame_width, frame_height), verts, normed=False)\n",
        "    pose_image_np = cv2.resize(lmk_img,  (width, height))\n",
        "    pose_list.append(pose_image_np)\n",
        "\n",
        "pose_list = np.array(pose_list)\n",
        "\n",
        "video_length = len(src_tensor_list)\n",
        "\n",
        "ref_image_tensor = pose_transform(ref_image_pil)  # (c, h, w)\n",
        "ref_image_tensor = ref_image_tensor.unsqueeze(1).unsqueeze(\n",
        "    0\n",
        ")  # (1, c, 1, h, w)\n",
        "ref_image_tensor = repeat(\n",
        "    ref_image_tensor, \"b c f h w -> b c (repeat f) h w\", repeat=video_length\n",
        ")\n",
        "\n",
        "src_tensor = torch.stack(src_tensor_list, dim=0)  # (f, c, h, w)\n",
        "src_tensor = src_tensor.transpose(0, 1)\n",
        "src_tensor = src_tensor.unsqueeze(0)\n",
        "\n",
        "video = pipe(\n",
        "    ref_image_pil,\n",
        "    pose_list,\n",
        "    ref_pose,\n",
        "    width,\n",
        "    height,\n",
        "    video_length,\n",
        "    args.steps,\n",
        "    args.cfg,\n",
        "    generator=generator,\n",
        ").videos\n",
        "\n",
        "video = torch.cat([ref_image_tensor, video, src_tensor], dim=0)\n",
        "\n",
        "# save_path = f\"{save_dir}/{ref_name}_{pose_name}_{args.H}x{args.W}_{int(args.cfg)}_{time_str}_noaudio.mp4\"\n",
        "save_path = f\"/content/noaudio.mp4\"\n",
        "save_videos_grid(\n",
        "    video,\n",
        "    save_path,\n",
        "    n_rows=3,\n",
        "    fps=src_fps if args.fps is None else args.fps,\n",
        ")\n",
        "\n",
        "probe = ffmpeg.probe(save_path)\n",
        "duration = float(probe['format']['duration'])\n",
        "output_file = \"/content/output.mp4\"\n",
        "audio_stream = ffmpeg.input(source_video_path).audio.filter('atrim', duration=duration)\n",
        "video_stream = ffmpeg.input(save_path)\n",
        "ffmpeg.output(video_stream, audio_stream, output_file).overwrite_output().run(overwrite_output=True)\n",
        "\n",
        "def cut_video_into_three_equal_parts(input_file):\n",
        "    (\n",
        "        ffmpeg\n",
        "        .input(input_file)\n",
        "        .filter('crop', 'iw/3', 'ih', x='0', y='0')\n",
        "        .output('/content/output_part1.mp4')\n",
        "        .run(overwrite_output=True)\n",
        "    )\n",
        "    (\n",
        "        ffmpeg\n",
        "        .input(input_file)\n",
        "        .filter('crop', 'iw/3', 'ih', x='iw/3', y='0')\n",
        "        .output('/content/output_part2.mp4')\n",
        "        .run(overwrite_output=True)\n",
        "    )\n",
        "    (\n",
        "        ffmpeg\n",
        "        .input(input_file)\n",
        "        .filter('crop', 'iw/3', 'ih', x='iw*2/3', y='0')\n",
        "        .output('/content/output_part3.mp4')\n",
        "        .run(overwrite_output=True)\n",
        "    )\n",
        "    audio_output = 'audio_from_video.aac'\n",
        "    ffmpeg.input(input_file).audio.output(audio_output, acodec='copy').run(overwrite_output=True)\n",
        "    audio = ffmpeg.input(audio_output)\n",
        "    video1 = ffmpeg.input('/content/output_part1.mp4')\n",
        "    video2 = ffmpeg.input('/content/output_part2.mp4')\n",
        "    video3 = ffmpeg.input('/content/output_part3.mp4')\n",
        "    ffmpeg.concat(video1, audio, v=1, a=1).output('/content/output_part1_a.mp4').run(overwrite_output=True)\n",
        "    ffmpeg.concat(video2, audio, v=1, a=1).output('/content/output_part2_a.mp4').run(overwrite_output=True)\n",
        "    ffmpeg.concat(video3, audio, v=1, a=1).output('/content/output_part3_a.mp4').run(overwrite_output=True)\n",
        "\n",
        "cut_video_into_three_equal_parts(output_file)\n",
        "\n",
        "from ipywidgets import Video\n",
        "Video.from_file(\"/content/output_part2.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
